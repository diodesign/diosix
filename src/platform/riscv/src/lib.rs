/* diosix RV32G/RV64G common hardware-specific code
 *
 * (c) Chris Williams, 2019.
 *
 * See LICENSE for usage and copying.
 */

/*!
# RISC-V platform

This platform crate is the Diosix hypervisor's RISC-V-specific code: it starts up a system, and subsequently controls the hardware on behalf
of the portable portion of the hypervisor, which lives in the [hypervisor](https://github.com/diodesign/diosix/blob/master/src/hypervisor/)
parent crate. This portable code should be free of hardware-specific routines and structures, and call down to a platform crate, such as this one,
to carry out hardware-specific actions, such as accessing timers and the serial port.
 
This platform crate is documented primarily to aid the porting of Diosix to other processor architectures, for example: OpenPOWER.
If a platform crate implements the following public data structures and methods, it should integrate seamlessly with the portable hypervisor, and
allow the hypervisor to run on hardware supported by the crate. A new platform crate effectively creates a hardware port of Diosix.

The portable hypervisor links with a platform crate when it is built, and that crate is chosen by the [`build.rs`](https://github.com/diodesign/diosix/blob/master/build.rs)
script from the `--target` parameter on the `cargo` command line. For example, selecting the target `riscv32imac-unknown-none-elf`
will link the portable hypervisor code with this `riscv` platform crate, producing an executable that runs on supported RISC-V-powered
hardware. This crate supports 32 and 64-bit RISC-V targets.

See the [build instructions](https://github.com/diodesign/diosix/blob/master/docs/building.md) for more details.

Feel free to drop the project lead a message if you have any questions. The next obvious port would be OpenPOWER.

# Implementing your own platform crate

The information below describes the interface that needs to be implemented between the portable hypervisor and a platform crate,
and suggests steps to create a new platform crate. The documentation should help you effectively port Diosix to more hardware. 
It is assumed you are comfortable writing low-level Rust and assembly code, and are familiar with stacks, interrupt handlers, linker scripts,
bootloaders, Device Trees, and similar concepts.

## Boot

A suitable GNU binutils-compatible linker script file - [link.ld](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/link.ld),
for example - should be used to to create a single executable, typically an ELF executable, that contains the portable hypervisor,
the platform-specific crate, a [boot capsule](https://github.com/diodesign/diosix/blob/master/docs/buildroot.md), and any required dependencies.
This executable should be loaded into memory and executed by a bootloader.

The linker script should define an entry point within the platform crate, and define locations in memory to load its various sections. These memory
addresses are largely specific to the hardware being supported, and should be chosen carefully by the crate's author. The platform crate's code is
the first Diosix code to run when a system is booted, specifically at the aforementioned entry point.

The crate's startup code, typically written in assembly language, should initialize one or more of the system's CPU cores, and assign them each a
stack of memory, set up suitable interrupt and exception handlers, and enable interrupts and exceptions. These handlers should be located within the
platform crate. Finally, the running core(s) should each call the portable hypervisor's entry point `hventry()` with the following parameters:

```no_run
pub extern "C" fn hventry(cpu_nr: CPUId, device_tree_buf: &u8)
```

where `cpu_nr` is an ID number representing the CPU core calling the `hventry()` method, and `device_tree_buf` is a pointer to a standard Device Tree Blob, a binary
data structure, that describes the hardware present in the system. This blob may be generated by the bootloader or system firmware, and thus can be passed
straight on to the hypervisor. The CPU ID number should be an unsigned integer unique to each CPU core, generated at boot and valid until the next system
boot. These ID numbers should start from zero and increase contiguously. 

One way to generate these ID numbers is to have each core atomically fetch, increment, and write back a shared unsigned integer value in memory, starting
with a value of zero. This ID number can also be used to calculate stack space for each CPU core: each core can use, for example, the `nth` 64KB block starting
from the base of a known available area of RAM, where `n` is the core's atomically assigned ID number, and the stack pointer of each core descends from
the top of each block. These stacks can be temporary, and long-term stacks allocated after the system has detected all present memory and CPU cores.

One CPU core, known as the boot core, must have an ID that matches `cpu::BOOT_CPUID`, which is defined in the hypervisor's
[`cpu.rs`](https://github.com/diodesign/diosix/blob/master/src/hypervisor/cpu.rs) as zero, as it will be used by the portable hypervisor to initialize the rest
of the system. Once the hypervisor's entry point `hventry()` is called by the boot core, and it has set up the hypervisor's memory management and other features,
all cores capable of running workloads will be made available to the scheduler, once they have each called `hventry()`. If a non-boot core calls `hventry()`
before this initialization is complete, it will be paused until the boot core completes its initialization work.

In summary, one or more CPU cores can call `hventry()` at any time to wait for workloads to be assigned to them by the scheduler. However, this assignment will not
begin until the boot core has called `hventry()` and successfully initialized the hypervisor. If `hventry()` returns to the startup code, a serious failure has
occurred, and the CPU core should be halted: an error message will be output by the hypervisor's debug subsystem. Note that the portable hypervisor's entry point,
`hventry()`, must only be called once per CPU core. In future, it may be possible to reenter the hypervisor if a failure occurs.

In this document, a workload is any software chosen to run on a CPU core: typically, that will be supervisor-mode or user-mode code running within a capsule,
which is an isolated virtualized environment, or the hypervisor itself, running in a mode below supervisor mode. The platform-specific code
leaves the definition and scheduling of workloads to the portable hypervisor: the platform crate simply runs what it is told to run.

## Exceptions and interrupts

Once a CPU core has called `hventry()`, the portable hypervisor has control of the core, and will direct the core to call down to the platform crate when
hardware-specific actions are required. However, when an exception or interrupt occurs, the relevant exception or interrupt handler within the platform crate,
specified by the startup code, will be called, passing control to the platform-specific crate. Note that the Diosix project refers to software-generated exceptions
and hardware-generated interrupts generically as IRQs, though this naming convention may change if it proves too confusing.

The IRQ handlers specified by the startup code must disable IRQs, if not already done so automatically by the CPU, and preserve the state of the interrupted
code's CPU registers onto a suitable stack. Then it must call up into the portable hypervisor's IRQ entry point:

```no_run
pub extern "C" fn hypervisor_irq_handler(context: IRQContext)
```

where `context` is a pointer to a platform-specific `IRQContext` structure describing the state of the CPU core when this particular IRQ was raised: this
should contain the state of the interrupted code, the cause of the IRQ, a return address, and other information from the platform that may be
needed to service the IRQ. The format of this structure is defined by the platform crate's implementor: it is opaque to the portable hypervisor.

The context should contain a snapshot of the interrupted software's CPU core and hardware state to a sufficient degree that this state can be
saved to memory and later restored from memory to the CPU core's registers, and the software resumed with any adverse effects: it should just continue as is.
A workload may be interrupted by a periodic timer interrupt, or it may make a system call, for instance. In either case, its context may be copied to memory,
and another workload's context loaded into the CPU registers from memory, continuing the latter software's execution.

The portable hypervisor, once `hypervisor_irq_handler()` is called, will decide what to do next, and call down into the platform crate to perform the next steps. This may involve
copying the interrupted code's register context from the IRQ handler's stack to a structure in memory, and replacing the context on the stack with another
workload's register context from memory.

Thus, when `hypervisor_irq_handler()` returns, whatever context is stored in the IRQ stack - be it the interrupted code's register state, or another
workload's register state - must be restored into the CPU core's registers, IRQs enabled again, and execution allowed to continue. This mechanism allows
the hypervisor to preemptively switch out and switch in different workloads for each CPU core, perform system calls, and so on, all transparently to the workloads.

The platform-specific code can optionally enable IRQs during the handling of an IRQ, provided it can cope with nested interrupts and exceptions. It is recommended
that exceptions, at least, are enabled in order to catch any errors.

## Steps to create a platform crate 

With the above in mind, the following steps are suggested as one approach to implementing a new platform crate:

### 1. Start afresh

Make a copy of this RISC-V crate, deleting the private structures and methods and the contents of the public methods until you are left with a
skeleton of empty public methods and populated public structures. Name the crate after the hardware platform, such as `openpower` or `arm`.

A platform crate should support a single processor architecture, such as Arm, RISC-V, MIPS, PowerPC, or x86. The crate should not be tied to a
particular system, such as a particular motherboard, and should instead be able to provide, or pass on from a bootloader or
system firmware, a Device Tree Blob describing the underlying memory map and peripherals that the hypervisor can process to discover and use hardware
as needed. This ensures a wide range of systems can be automatically supported, rather than hard-wire support for a select few into the codebase.

### 2. Craft a linker script and boot code

Write a GNU binutils-compatible linker script that defines an entry point, such as `_start`, and be aware this may need to be at a particular address
in memory, depending on the bootloader and firmware. Create an `asm/entry.s` assembly language source file, and in it, implement `_start`.
This routine should assign any running cores with unique ID numbers, as described above, and some individual initial stack space. Next, it should
perform any required initialization, as described by the processor architecture's manuals, and then configure the CPU cores to use interrupt and
exception handlers you will define in `asm/irq.s`.

Finally, enable interrupts, and call `hventry()` with the ID number and a pointer to a Device Tree Blob describing the system. This blob will need
to be generated either by the boot code, the bootloader, or the machine's firmware.

### 3. Implement IRQ handlers

In `asm/irq.s`, construct the software exception and hardware interrupt handlers specified in `entry.s`. Each of these handlers should preserve
the interrupted code's CPU registers and any other relevant hardware context onto a stack. Once this state is saved, `hypervisor_irq_handler()`
should be called with a pointer to that stacked information, which is presented to the hypervisor as an opaque platform-specific `IRQContext`
structure. When `hypervisor_irq_handler()` returns, the handler should unwind its previous actions: it should copy the stacked context back into
the CPU's registers, enable IRQs, and return to whatever code was interrupted.

The context on the stack may have been switched for the context of another workload, in which case the CPU is actually returning to different code.
Some of the context may also have been altered if the workload made a system call, and data is being returned via the CPU registers.

The portable portion of the hypervisor cannot understand the machine-specific `IRQContext` so after it is invoked via `hypervisor_irq_handler()`,
it will call the following platform-provided public method, implemented in, say, `src/irq.rs` to understand the IRQ:

```no_run
pub fn dispatch(context: IRQContext) -> IRQ
```

where `context` is the context passed to `hypervisor_irq_handler()` and `IRQ` is a portable representation of an interrupt or exception. This
method's job is to translate the machine-specific `context` to an `IRQ` structure the hypervisor can understand regardless of the underlying
hardware. See the definition of `IRQ` in [`src/irq.rs`](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/irq.rs) for a guide,
or the documentation below.

Finally, in `src/irq.rs`, implement the following public method:

```no_run
pub fn acknowledge(irq: IRQ)
```

which should instruct the source of the given hardware IRQ, `irq`, that its latest interrupt has been serviced, and that it should not generate
another IRQ unless a fresh interrupt condition arises. This method is called from `hypervisor_irq_handler()` as it concludes handling a hardware interrupt.

### 4. Implement CPU management code

Various methods for inspecting and controlling CPU cores, in [`src/cpu.rs`](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/cpu.rs),
need to be implemented. These are called from the portable hypervisor when it needs to perform platform-specific tasks, and most of them should be
straightforward to port.

Of interest is `save_supervisor_state()`, which copies the currently running workload's register and hardware context, stored on the CPU core's IRQ stack,
to a structure in memory, and `load_supervisor_state()` overwrites the currently running workload's context, stored on the CPU core's IRQ stack, with
another context from memory.

These are required to context switch between workloads. Each workload can have one or more virtual CPUs assigned to it; these are essentially
traditional threads if you consider a workload a process. The scheduler switches workload virtual CPU contexts in and out of the physical CPUs. This is
all managed from the portable side of the hypervisor: the platform crate has to implement the actual preservation and restoration of CPU contexts.

Meanwhile, `CPUDescriptionIter` should iterate over the present physical CPU cores, and describe each of them as a text string. Finally,
`features_priv_check()` checks whether the CPU core can run code in the given processor mode, such as user mode. This is needed to support
architectures that provide CPU cores, such as high-privilege system management cores, that are only capable of running code in a particular level.

### 5. Implement hardware support

Finally, the platform crate will need to provide support for [timers](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/timer.rs),
a [serial port](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/serial.rs) for debug output,
[ending unit tests](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/test.rs) in Qemu, and accessing and controlling
[physical memory](https://github.com/diodesign/diosix/blob/master/src/platform/riscv/src/physmem.rs). These are hardware specific and beyond the scope
of this porting overview.

**The specifics of the methods and structures that need to be implemented will be described in the reference documentation below.**

*/

#![no_std]
#![feature(asm)]

/* expose architecture common code to platform-specific code */
#[macro_use]
pub mod csr;
pub mod physmem;
pub mod devicetree;
pub mod irq;
pub mod cpu;
pub mod timer;
pub mod serial;
pub mod test;